---
title: "Midterm Exam Solution Script"
author: "Your Name - Your Student ID"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: readable
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
# This chunk sets up the R Markdown environment.
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# 1. Initial Setup

This section clears the environment and loads all the libraries needed for the exam. Run this chunk first.

```{r initial_setup}
# Clear all variables from the workspace
rm(list=ls())

# Load all necessary libraries
library(ggplot2)  # For plotting
library(caTools)  # For train/test splits (sample.split)
library(ROCR)     # For ROC curves and AUC
library(leaps)    # For best subset selection (regsubsets)
library(MASS)     # For LDA, QDA, and the Boston dataset
library(psych)    # For pairs.panels()
```

------------------------------------------------------------------------

# Problem 1: [Example with `Boston` Dataset for Linear Regression]

## (a) Load and Inspect Data

```{r p1_load}
# Load the Boston dataset from the MASS library
data(Boston)

# Use a clear name for your data
my_data <- Boston

# Initial inspection
str(my_data)
summary(my_data)
```

**Answer for (a):** \* [Write your findings here based on `str()` and `summary()`. Example: "The dataset has 506 observations and 14 variables. The response variable `medv` has a mean of 22.53."]

## (b, c, d) Data Exploration & Hypothesis Testing

```{r p1_explore_ttest}
# Example: Compare medv for houses near the Charles River (chas=1) vs. not (chas=0)
group1 <- subset(my_data, chas == 1)
group2 <- subset(my_data, chas == 0)

# Calculate grouped means
cat("Mean medv for homes near river:", mean(group1$medv), "\n")
cat("Mean medv for homes away from river:", mean(group2$medv), "\n")

# Perform a t-test
ttest_result <- t.test(group1$medv, group2$medv)
print(ttest_result)
```

**Answer for (b):** \* [Write your calculated means here.]

**Answer for (c):** \* Null Hypothesis (Hâ‚€): The true mean `medv` is the same for houses near the river and houses away from the river.

**Answer for (d):** \* P-value: `r ttest_result$p.value`. \* Conclusion: Since the p-value is less than 0.05, we reject the null hypothesis and conclude there is a significant difference in median value.

## (e, f, g) Simple Linear Regression & Prediction

```{r p1_simple_lm}
# Create training and test sets
set.seed(1)
split <- sample.split(my_data$medv, SplitRatio = 0.8)
train_set <- subset(my_data, split == TRUE)
test_set <- subset(my_data, split == FALSE)

# Fit a simple linear model on the training data
model_simple <- lm(medv ~ lstat, data = train_set)
cat("R-squared for simple model:", summary(model_simple)$r.squared, "\n")

# Predict on a specific new data point with a confidence interval
# Let's predict for the first row of the test set
specific_case <- test_set[1, ]
prediction_ci <- predict(model_simple, newdata = specific_case, interval = "confidence", level = 0.99)
print(prediction_ci)
```

**Answer for (e):** \* R-squared: `r summary(model_simple)$r.squared`.

**Answer for (f):** \* Confidence Interval: `[lwr, upr]` from the prediction output. \* Is the actual value (`test_set[1, "medv"]`) within the interval? [Yes/No].

**Answer for (g):** \* [Repeat for another case if needed.]

## (h, i, j) Multiple Linear Regression & Model Comparison

```{r p1_multi_lm}
# Fit a model with all predictors on the training data
model_multi_all <- lm(medv ~ ., data = train_set)
# summary(model_multi_all) # Run this to see significant variables

# For this example, let's say lstat, rm, and ptratio were significant
model_multi_sig <- lm(medv ~ lstat + rm + ptratio, data = train_set)
# summary(model_multi_sig)

# Compare models using Adjusted R-squared (higher is better)
cat("Adj. R-squared for Simple Model:", summary(model_simple)$adj.r.squared, "\n")
cat("Adj. R-squared for Full Model:", summary(model_multi_all)$adj.r.squared, "\n")
cat("Adj. R-squared for Significant-Only Model:", summary(model_multi_sig)$adj.r.squared, "\n")
```

**Answer for (h):** \* Significant variables from the full model are... [List them from `summary(model_multi_all)`].

**Answer for (i):** \* Adjusted R-squared for the significant-only model is... [Value from `summary()`].

**Answer for (j):** \* Preferred Model: Based on Adjusted R-squared, the `model_multi_all` is the best among these three.

## (k, l, m) Automated Subset Selection

```{r p1_subset_selection}
# Best subset selection on the training data
model_subsets <- regsubsets(medv ~ ., data = train_set, nvmax = 13)
summary_subsets <- summary(model_subsets)

# Find best model size by Adjusted R-squared
best_adjr2_size <- which.max(summary_subsets$adjr2)
cat("Best model size by Adj R-squared:", best_adjr2_size, "\n")

# Find best model size by BIC
best_bic_size <- which.min(summary_subsets$bic)
cat("Best model size by BIC:", best_bic_size, "\n")
```

**Answer for (k), (l), (m):** \* The best model according to Adjusted R-squared has `r best_adjr2_size` variables. \* The best model according to BIC has `r best_bic_size` variables.

## (n, o, p) Out-of-Sample Validation (Test Set Error)

```{r p1_test_error}
# This is the final validation. The model with the lowest Test Set Error wins.
# We will calculate the RMSE (Root Mean Squared Error) for each model.

# Function to calculate test RMSE for a given model size from regsubsets
get_test_rmse <- function(model, train_data, test_data, id) {
  # Get predictor names from the best model
  predictors <- names(coef(model, id = id))[-1]
  # Build the formula
  formula <- as.formula(paste("medv ~", paste(predictors, collapse="+")))
  # Fit the model ON THE TRAINING DATA
  final_model <- lm(formula, data = train_data)
  # Predict ON THE TEST DATA
  predictions <- predict(final_model, newdata = test_data)
  # Calculate and return RMSE
  error <- sqrt(mean((predictions - test_data$medv)^2))
  return(error)
}

# Calculate Test RMSE for models selected by Adj R-squared and BIC
rmse_adjr2 <- get_test_rmse(model_subsets, train_set, test_set, id = best_adjr2_size)
rmse_bic <- get_test_rmse(model_subsets, train_set, test_set, id = best_bic_size)

cat("Test RMSE for model selected by Adj R-squared:", rmse_adjr2, "\n")
cat("Test RMSE for model selected by BIC:", rmse_bic, "\n")
```

**Answer for (n), (o), (p):** \* Test RMSE for Adj R-squared model: `r rmse_adjr2`. \* Test RMSE for BIC model: `r rmse_bic`. \* Preferred Model: The model selected by **BIC** is preferred because it has a lower test set error.

------------------------------------------------------------------------

# Problem 2: [Example with `Default` Dataset for Classification]

## (a, b, c) Load Data & Create Train/Test Splits

```{r p2_load_split}
# Load data from ISLR
library(ISLR)
data(Default)

# Create training and test sets
set.seed(1)
split <- sample.split(Default$default, SplitRatio = 0.7)
train_set2 <- subset(Default, split == TRUE)
test_set2 <- subset(Default, split == FALSE)
```

**Answer for (a), (b), (c):** \* [Write findings here. e.g., "The training set has `r nrow(train_set2)` observations."]

## (d) Fit Initial Logistic Model

```{r p2_initial_glm}
# Fit the logistic regression model
model1_glm <- glm(default ~ ., data = train_set2, family = "binomial")
cat("AIC for full logistic model:", AIC(model1_glm), "\n")
```

**Answer for (d):** \* AIC: `r AIC(model1_glm)`.

## (f, g, h) Multicollinearity & Model Refinement

```{r p2_multicollinearity}
# Check correlation between suspected variables
# In the Default dataset, predictors are not highly correlated.
# For demonstration, let's compare the full model to a simpler one.
# summary(model1_glm) # Shows income is not significant

# Model 2 (remove income)
model2_glm <- glm(default ~ . - income, data = train_set2, family = "binomial")
cat("AIC for model without income:", AIC(model2_glm), "\n")
```

**Answer for (f), (g), (h):** \* The AIC for the model without `income` is `r AIC(model2_glm)`, which is slightly lower (better) than the full model, confirming that `income` was not a useful predictor.

## (i, j) Prediction & Evaluation on Test Set

```{r p2_evaluation}
# Make predictions ON THE TEST SET using the better model (Model 2)
predictions <- predict(model2_glm, newdata = test_set2, type = "response")

# Create confusion matrix with a 0.40 threshold
conf_matrix <- table(Actual = test_set2$default, Predicted = predictions > 0.40)
print(conf_matrix)

# Calculate Model Accuracy
accuracy <- (conf_matrix[1,1] + conf_matrix[2,2]) / sum(conf_matrix)
cat("Model Accuracy:", accuracy, "\n")

# Calculate Baseline Accuracy (always predict the majority class)
baseline_accuracy <- mean(test_set2$default == "No")
cat("Baseline Accuracy:", baseline_accuracy, "\n")

# Calculate Sensitivity and Specificity
sensitivity <- conf_matrix[2,2] / (conf_matrix[2,1] + conf_matrix[2,2])
specificity <- conf_matrix[1,1] / (conf_matrix[1,1] + conf_matrix[1,2])
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
```

**Answer for (i), (j):** \* [Write your calculated Accuracy (`r accuracy`), Baseline Accuracy (`r baseline_accuracy`), Sensitivity (`r sensitivity`), and Specificity (`r specificity`).]

## (k) Calculate AUC

```{r p2_auc}
# Use the predictions from the previous chunk
pred_rocr <- prediction(predictions, test_set2$default)
auc_performance <- performance(pred_rocr, "auc")
auc_value <- auc_performance@y.values[[1]]
```

**Answer for (k):** \* AUC Value: `r auc_value`.

## (l, m, n) Alternative Model (e.g., Probit) & Final Comparison

```{r p2_probit}
# Let's fit the Probit model with the same predictors as our best logistic model
model3_probit <- glm(default ~ . - income, data = train_set2, family = binomial(link = "probit"))

# Predict and evaluate Model 3
probit_predictions <- predict(model3_probit, newdata = test_set2, type = "response")
probit_conf_matrix <- table(Actual = test_set2$default, Predicted = probit_predictions > 0.40)
probit_accuracy <- (probit_conf_matrix[1,1] + probit_conf_matrix[2,2]) / sum(probit_conf_matrix)
cat("Probit Model Accuracy:", probit_accuracy, "\n")
```

**Answer for (l), (m), (n):** \* The Probit model's accuracy is `r probit_accuracy`, which is very similar to the logistic model. Given their nearly identical performance, either model could be chosen, but logistic regression is often preferred due to its more direct interpretation in terms of odds ratios. \`\`\`
