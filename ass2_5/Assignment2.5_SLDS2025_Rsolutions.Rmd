---
title: "(Practice) Assignment 2.5"
subtitle: "Statistical Learning for Data Science"
date: "2025"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    toc: yes
    toc_depth: 2
    number_sections: yes
---


#  Question 1
## (a) {#onea}
Q: Run a logit model with installation cost and operating cost as the only explanatory variables, without intercepts.

A: We need to prepare the data for `mlogit()` using `mlogit.data()` first. The most important column is the column of choices, which we will use to get all of the other columns. We note that we assume that the data has been prepared and that the columns have the alternative names within them.

We just read in the data first:
```{r}
rm(list=ls())
#install.packages(mlogit)
library(mlogit)
heating <- read.csv("Heating.csv")
head(heating)
```

For creating the required data we use the function mlogit.data, the different arguments are explained on the side.
We need to choose columns 3 to 12 for the explanatory variables. This is also a wide data set.
```{r}
dataheat <- mlogit.data(heating,  # data.frame of data
                    choice = "depvar",  # column name of choice
                    shape = "wide",  # wide means each row is an observation
                                     # long if each row is an alternative
                    varying = c(3:12), # indices of varying columns for each alternative,
                    sep = "."  # not necessary but still good to be clear
                    )
```

Then, we can run `mlogit()` on the data:
```{r}
modelQ1_1 <- mlogit(depvar ~ ic + oc - 1, dataheat)  # -1 means no intercept
```


### i.
Q: Do the estimated coefficients have the expected signs?

A: 
```{r}
coef(modelQ1_1)
```
The coefficients of both `ic` and `oc` are negative which makes sense since as the installation cost and operating cost for a system increases, the probability of choosing that system goes down.

### ii.
Q: Are both coefficients significantly different from zero?

A: Since both p-values are below 2.2e-16 and we see three stars, we can claim that the coefficients are significantly different than zero.
```{r}
summary(modelQ1_1)
```


### iii.
Q: Use the average of the probabilities to compute the predicted share. Compute the actual shares of houses with each system. How closely do the predicted shares match the actual shares of houses with each heating system?

A: 
We can get the actual shares in the data with the following code:
```{r}
table(heating$depvar)/nrow(heating)

predQ1_1<- predict(modelQ1_1, newdata = dataheat)
apply(predQ1_1, 2, mean)
```

While the model captures the essence of the data reasonably, there are a few differences in the predicted shares. For example in `gc` and `gr` there seems to be quite a gap.

### iv. 
Q:  The ratio of coefficients usually provides economically meaningful information in discrete choice models. The willingness to pay (*wtp*) through higher installation cost for a one-dollar reduction in operating costs is the ratio of the operating cost coefficient to the installation cost coefficients. What is the estimated *wtp* from this model? Note that the annual operating cost recurs every year while the installation cost is a one-time payment. Does the result make sense?

A: 
\begin{equation*}
\frac{\beta_{oc}}{\beta_{ic}}=`r coef(modelQ1_1)["oc"]/coef(modelQ1_1)["ic"]`
\end{equation*}


According to this model, the decision-makers are willing to pay \$ 0.739 higher in installation cost for a \$1 reduction in operating cost. It seems unreasonable for the decision-maker to pay only 74 cents higher for a one-time payment for a \$1 reduction in annual costs.

```{r}
wtp1 <- as.numeric(coef(modelQ1_1)["oc"]/coef(modelQ1_1)["ic"])
wtp1
```



## (b) {#oneb}
Q:  The present value ($PV$) of the future operating costs is the discounted sum of operating costs over the life of the system: $PV=\sum_{t=1}^{L}[OC/(1+r)^{t}]$ where *r* is the discount rate and *L* is the life of the system. As *L* rises, the PV approaches *OC/r*. Therefore, for a system with a sufficiently long life (which we will assume these systems have), a one-dollar reduction in *OC* reduces the present value of future operating costs by *(1/r)*. This means that if the person choosing the system were incurring the installation costs and the operating costs over the life of the system, and rationally traded-off the two at a discount rate of *r*, the decision-makerâ€™s *wtp* for operating cost reductions would be *(1/r)*. Define a new variable `lcc` (lifecycle cost) that is defined as the sum of the installation cost and the (operating cost)/*r*. Run a logit model with the lifecycle cost as the only explanatory variable. Estimate the model for r = 0.12.  Comment on the value of log-likelihood of the models obtained in [(a)](#onea) as compared to [(b)](#oneb).

A: We first make a column called `lcc` with our `dataheat` object
```{r}
dataheat$lcc <- dataheat$ic + dataheat$oc/0.12
```
We then estimate with the `mlogit()` function, and call `logLik()` to get the log likelihood for this model:
```{r 1_b2}
modelQ1_2 <- mlogit(depvar ~ lcc - 1, dataheat)
logLik(modelQ1_2)
```

The log likelihood of the model is `r logLik(modelQ1_2)`.

For comparison, we retrieve the log likelihood of the model in part (a):
```{r 1_b3}
logLik(modelQ1_1)
```

The log likelihood of the model from (a) is `r logLik(modelQ1_1)`.

Notice that the log likelihood of the model in (a) is higher (better, more likely) than that of the model in this part. 


## (c) {#onec}
Q: Add alternative-specific constants to the model in (a). With *K* alternatives, at most *K-1* alternative specific constants can be estimated. The coefficient of *K-1* constants are interpreted as relative to *K*th alternative. Normalize the constant for the alternative `hp` to 0.


A: Running `mlogit()` with a reference level. We observe that the share obtained here is the average share. This is guaranteed by the presence of alternative specific constants.
```{r}
modelQ1_3 <- mlogit(depvar ~ ic + oc, data = dataheat, reflevel = "hp")
# This forces hp to be the reference level and the other alternative specific constants are relative to this
```

### i.
Q: How well do the estimated probabilities match the shares of customers choosing each alternative in this case?

A: We can get the predicted shares:
```{r 1_c_i}
predQ1_3<- predict(modelQ1_3, newdata=dataheat)
shareQ1_3<- apply(predQ1_3,2,mean)
shareQ1_3
```

We notice that the predicted shares match the actual shares exactly. This is guaranteed with the use of the alternative specific constants.

### ii.
Q: Calculate the *wtp* that is implied by the estimate. Is this reasonable?

A:
We calculate the willingness to pay:
```{r 1_c_ii}
unname(modelQ1_3$coefficients["oc"]/modelQ1_3$coefficients["ic"])
```
Hence:
\begin{equation*}
\frac{\beta_{oc}}{\beta_{ic}}=`r unname(modelQ1_3$coefficients["oc"]/modelQ1_3$coefficients["ic"])`
\end{equation*}
which suggests an extra down-payment of \$ 4.56 for a \$1 saving in annual operating costs. This seems more reasonable.

### iii.
Q: Suppose you had included constants for alternatives `ec`, `er`, `gc`, `hp` with the constant for alternative `gr` normalized to zero. What would be the estimated coefficient of the constant for alternative `gc`? Can you figure this out logically rather than actually estimating the model?


A: Note that in  modelQ1_3, the intercept for `gr` is 0.308. Here `gr` is the reference level. So in teh new model all the alternative specific constants are reduced by 0.308. Nothing else chages and the quality of fit remains unchanged.
```{r}
summary(modelQ1_3)

### Check that you are right.
modelQ1_4 <- mlogit(depvar ~ ic + oc, data = dataheat, reflevel = "gr")
summary(modelQ1_4)

```


## (d)
Now try some models with socio-demographic variables entering.

### i.
Q: Enter installation cost divided by income, instead of installation cost. With this specification, the magnitude of the installation cost coefficient is inversely related to income, such that high-income households are less concerned with installation costs than lower-income households. Does dividing installation cost by income seem to make the model better or worse than the model in [(c)](#onec)?


A: Fitting the model first
```{r}
dataheat$iic <- dataheat$ic/dataheat$income
modelQ1_5 <- mlogit(depvar ~ oc + iic, dataheat)
summary(modelQ1_5)
```

The log-likelihood here is -1010.2 which is lower than -1008.2 for model4 and hence is worse. Moreover installation cost was significant in the earlier model and here the installation cost divided by income is not significant any more.


### ii.
Q: Instead of dividing installation cost by income, enter alternative-specific income effects. You can do this by using the `|` argument in the mlogit formula. What do the estimates imply about the impact of income on the choice of central systems versus room system? Do these income terms enter significantly?

A:
```{r}
modelQ1_6 <- mlogit(depvar ~ oc + ic | income, dataheat)  
summary(modelQ1_6)
```
All of the coefficients are negative which tells us that income rises, probability of choosing a heat pump increases relative to others, The magnitude of the income coefficient for `gr` is the greatest so we can infer that as income rises, probability of choosing gas rooms drops relative to others.
None of the income terms are significant at the 5% significance level.

## (e)
Q: We now are going to consider the use of the logit model for prediction. Estimate a model with installation costs, operating costs, and alternative specific constants. Calculate the probabilities for each house explicitly.

### i.
Q: The California Energy Commission (CEC) is considering whether to offer rebates on heat pumps. The CEC wants to predict the effect of the rebates on the heating system choices of customers in California. The rebates will be set at 10% of the installation cost. Using the estimated coeffiients from the model, calculate predicted shares under this new installation cost instead of original value. How much do the rebates raise the share of houses with heat pumps?

A: We create a new dataframe via copying and then changing a column, and then create a new `mlogit.data` object:
```{r}
heating1 <- heating
heating1$ic.hp <- 0.9*heating1$ic.hp
dataheat1 <- mlogit.data(heating1, choice = "depvar", shape = "wide", varying = c(3:12))
```


We can then use the old model as-is with the newly created data:
```{r}
predQ1_3a <- predict(modelQ1_3, newdata = dataheat1)
shareQ1_3a <- apply(predQ1_3a, 2, mean)
shareQ1_3a
```

The share of houses with heat pumps rises from 0.055 to 0.0645.

### ii.
Q: Suppose a new technology is developed that provides more efficient central heating. The new technology costs 200 more than the electric central heating system. However it saves 25% of the electricity such that its operating costs are 75% of the operating costs of `ec`. We want to predict the original market penetration of this technology. Note that there are now 6 alternatives instead of 5. Calculate the probability and predict the market share (average probability) for all 6 alternatives using the model that is estimated on the 5 alternatives. Use the original installation costs for the heat pumps rather than the reduced costs from the previous question. What is the predicted market share for the new technology? From which of the original five systems does the new technology draw the most customers?


A: We want to compute the choice probabilities using closed form formula:
\begin{equation*}
\mathbb{P}\left(\text{Choice $k$=$j$}\right)=\frac{\exp\left(\sum_{i\in \mathcal{I}}\beta_{ij}x_{ij}\right)}{\sum_{l\in\mathcal{J}}\exp\left(\sum_{i\in \mathcal{I}}\beta_{il}x_{il}\right)}
\end{equation*}
where $i$ refers to each predictor, including the intercept. The intercept always has corresponding $x_{0j}$ value of 1, and coefficient of the intercept $\beta_{0j}$ for the reference level is 0. The indices $j$ refers to each choice or alternative while $k$ can be treated as the choice random variable and takes the values the choices are encoded as.

Unfortunately, first we will need to introduce columns that the new choice represents, and then we can calculate this.


```{r}
df<- subset(heating, select = c(3:12))

# New columns
df$ic.eci <- df$ic.ec + 200
df$oc.eci <- df$oc.ec * 0.75

```

We will use modelQ1_3 to compute the new choice probabilities
```{r}
df$hpexp<-exp(modelQ1_3$coefficients["oc"]*df$oc.hp+modelQ1_3$coefficients["ic"]*df$ic.hp)

df$ecexp<-exp(modelQ1_3$coefficients["oc"]*df$oc.ec+modelQ1_3$coefficients["ic"]*df$ic.ec+modelQ1_3$coefficients["(Intercept):ec"])

df$erexp<-exp(modelQ1_3$coefficients["oc"]*df$oc.er+modelQ1_3$coefficients["ic"]*df$ic.er+modelQ1_3$coefficients["(Intercept):er"])

df$gcexp<-exp(modelQ1_3$coefficients["oc"]*df$oc.gc+modelQ1_3$coefficients["ic"]*df$ic.gc+modelQ1_3$coefficients["(Intercept):gc"])

df$grexp<-exp(modelQ1_3$coefficients["oc"]*df$oc.gr+modelQ1_3$coefficients["ic"]*df$ic.gr+modelQ1_3$coefficients["(Intercept):gr"])

df$eciexp<-exp(modelQ1_3$coefficients["oc"]*df$oc.eci+modelQ1_3$coefficients["ic"]*df$ic.eci+modelQ1_3$coefficients["(Intercept):ec"])
               
               

df$sumexp <-apply(subset(df,select=c(13:17)),1,sum)
df$sumexpnew <-apply(subset(df,select=c(13:18)),1,sum)


df$hp <-df$hpexp/df$sumexp
df$ec <-df$ecexp/df$sumexp
df$er <-df$erexp/df$sumexp
df$gc <-df$gcexp/df$sumexp
df$gr <-df$grexp/df$sumexp

df$hpnew <-df$hpexp/df$sumexpnew
df$ecnew <-df$ecexp/df$sumexpnew
df$ernew <-df$erexp/df$sumexpnew
df$gcnew <-df$gcexp/df$sumexpnew
df$grnew <-df$grexp/df$sumexpnew
df$ecinew <-df$eciexp/df$sumexpnew




oldprob<-subset(df,select=c(21:25))
newprob<-subset(df,select=c(26:31))


marketshareold<-apply(oldprob,2,mean)


marketsharenew<-apply(newprob,2,mean)


marketshareold

marketsharenew

```
\pagebreak

The new technology is predicted to have a marketshare of about 10.3\%

The most market share is drawn from gas central whose marketshare falls from 63.67\% to 57.15\%. Note that from the independence of irrelevent alternatives (IIA) property, the ratio of market shares remains the same irrespective of other alternatives in the set. The drop in percentage is roughly 10\% from each system due to IIA. It might have been expected that the new electric central heating system would possibly draw more from the old electric central heating system rather than gas central (which just happens to have the greatest market share) but the multinomial logit with the IIA property is unable to account for this.



\pagebreak


# Question 2
Suppose we perform best subset, forward stepwise, and backward stepwise selection on a single set. For each approach, we obtain *p*+1 models, containing 0, 1, 2, ..., *p* predictors. Provide your answers for the following questions:

## (a)
Q: Which of the three models with *k* predictors has the smallest training sum of squared errors?

A: By definition, the best subset selection would select a subset of the predictors that would minimize training sum of squared errors, for any *k*.

## (b)
Q: Which of the three models with *k* predictors has the smallest test sum of squared errors?

A: This is impossible to say as information of the test set is not considered in any of the three methods named. Fitting well on the training set does not necessarily generalize to fitting well on the test set.

## (c)
Q: Are the following statements **True** or **False**:

### i.
Q:  The predictors in the *k*-variable model identified by forward stepwise selection are a subset of the predictors in the (*k*+1)-variable model identified by forward stepwise selection.

A: True. Each step in the forward stepwise selection method corresponds to adding only 1 variable to the previous set, typically in greedy-like manner, and removals are never done.

### ii.
Q: The predictors in the *k*-variable model identified by backward stepwise selection are a subset of the predictors in the (*k*+1)-variable model identified by backward stepwise selection.

A:  True. In backward stepwise selection, we drop 1 variable at each step.

### iii.
Q: The predictors in the *k*-variable model identified by backward stepwise selection are a subset of the predictors in the (*k*+1)-variable model identified by forward stepwise selection.

A: False. This is clearly not true since forward and backward selection may collect a very different subset of sizes (*k*+1) and *k* respectively

### iv.
Q: The predictors in the *k*-variable model identified by forward stepwise selection are a subset of the predictors in the (*k*+1)-variable model identified by backward stepwise selection.

A: False. For a similar reason as mentioned in part (iii).

### v.
Q: The predictors in the *k*-variable model identified by best stepwise selection are a subset of the predictors in the (*k*+1)-variable model identified by best stepwise selection.
 
A: False. For a similar reason as mentioned in part (iii).


\pagebreak

# Question 3
## (a)
Q:  Split the data set into a training set and a test set using the seed 1 and the `sample()` function with 80% in the training set and 20% in the test set. How many observations are there in the training and test sets?

A:
```{r}
college <- read.csv("College.csv")
# str(college)
set.seed(1)
trainid <- sample(1:nrow(college), 0.8*nrow(college))
testid <- -trainid
train <- college[trainid,]
test <- college[testid,]
nrow(train) #80%
nrow(test) #20%
```
There are `r nrow(train)` observations in the training set, and `r nrow(test)` observations in the test set.

## (b) {#fourb}
Q: Fit a linear model using least squares on the training set. What is the average sum of squared error of the model on the training set? Report on the average sum of squared error on the test set obtained from the model.

A:
```{r}
modelQ4_1 <- lm(Apps ~ ., data = train)

#summary(modelQ4_1)

# sum squared error on training data
SSE_tr <- mean(modelQ4_1$residuals^2) 
SSE_tr

predQ4_1 <- predict(modelQ4_1, newdata = test)

# sum squared error on test data
SSE_te <- mean((test$Apps - predQ4_1)^2) 
SSE_te
```

The average sum of squared error on the training set is 958950.3 while the average sum of squared error on the test set is 1567324.

## (c)
Q: Use the backward stepwise selection method to select the variables for the regression model on the training set. Which is the first variable dropped from the set?

A:
```{r}
library(leaps)
modelQ4_2sub <- regsubsets(Apps~., data = train,
                           nvmax = NULL,  # alternatively, 17
                           method = "backward")
summary(modelQ4_2sub)
```

From the results we can see that the first variable to be dropped from the set is `Terminal`.

## (d) {#fourd}
Q: Plot the adjusted-$R^{2}$ for all these models. If we choose the model based on the best adjusted-$R^{2}$ value, which variables should be included in the model?

A: The plot are done as follows
```{r}
plot(summary(modelQ4_2sub)$adjr2)

# best step number (model with highest adjusted r^2) 
which.max(summary(modelQ4_2sub)$adjr2)
n_vars <- which.max(summary(modelQ4_2sub)$adjr2) # number of variables to be included 
coef_vars <- coef(modelQ4_2sub, n_vars) # coefficients of variables to be included
sel_vars <- names(coef_vars) # names of variables to be included (includes intercept)
sel_vars
```

The model with the best adjusted-$R^{2}$ is the model with `r n_vars`  variables which are `PrivateYes, Accept, Enroll`, `Top10perc, Top25perc, F.Undergrad, P.Undergrad`, `Outstate, Room.Board, PhD, S.F.Ratio, Expend, Grad.Rate` along with the `Intercept` term.
 The variables `Books, Terminal, perc.alumni, Personal` are dropped from the model.


## (e)
Q:  Use the model identified in part [(d)](#fourd) to estimate the average sum of squared test error. Does this improve on the model in part [(b)](#fourb) in the prediction accuracy?

A: We first build the model using the selected variables then find the average sum of squared test error:
```{r}
# note: in our dataset, only 'Private' variable exists ('PrivateYes' was created by the model since 'Private' was a factor variable)
sel_vars <- sub("PrivateYes","Private",sel_vars) # replace privateyes with private
sel_vars <- sel_vars[2:n_vars] # remove the '(Intercept)' from sel_vars
sel_vars # we use this to create our linear model

# instead of typing the variables manually, use paste and collapse function to create string: "App ~ var1 + var2 + ... + ..."
modelQ4_3 <- lm(
  paste("Apps",
        paste(sel_vars,
              collapse = " + "),
        sep = " ~ "),
  data = train)

summary(modelQ4_3)

predQ4_3 <- predict(modelQ4_3, newdata = test)

SSE_te_d <- mean((test$Apps - predQ4_3)^2) # using model in d
SSE_te_d
```
The test MSE is 1588185 which is more than 1567324, so the new model seems to reduce prediction accuracy.

## (f)
Q: Fit a LASSO model on the training set. Use the command to define the grid for $\lambda$:

`grid <- 10^seq(10, -2, length = 100)`

Plot the behavior of the coefficients as $\lambda$ changes.

A: First we can initialize the `grid` then run `glmnet` to fit the LASSO model with differing $\lambda$ values:
```{r 4_f1}
library(glmnet)
# define grid for lambda
grid <- 10^seq(10, -2, length = 100)

Xglm <- model.matrix(Apps~., college)
yglm <- college$Apps
modelQ4_4 <- glmnet(Xglm[trainid,], yglm[trainid], lambda = grid)
```

Then we plot,
```{r}
plot(modelQ4_4, xvar = "lambda")
```

## (g)
Q: Set the seed to 1 before running the cross-validation with LASSO to choose the best $\lambda$. Use 10-fold cross validation. Report the test error obtained, along with the number of non-zero coefficient estimates.

A:
```{r}
set.seed(1)
cvmodelQ4_4 <- cv.glmnet(x = Xglm[trainid,], y = yglm[trainid],
                         nfolds = 10, lambda = grid)  # 10-fold cross-validation
cvmodelQ4_4$lambda.min

coef(modelQ4_4,s=cvmodelQ4_4$lambda.min)
cvmodelQ4_4$glmnet.fit

predQ4_4 <- predict(modelQ4_4,
                    s=cvmodelQ4_4$lambda.min,
                    newx=Xglm[testid,])

te <- mean((predQ4_4 - yglm[testid])^2)  # test error of best model
te
```

The number of non-zero coefficients is 17. This means that the model is essentially the same as the model in part [(b)](#fourb) which is the full model. The test error of 1565789 is approximately the same.